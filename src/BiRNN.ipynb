{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stress Determinator\n",
    "\n",
    "### Bi-directional Long-Short-Term-Memory\n",
    "\n",
    "#### Implemented by Pytorch\n",
    "\n",
    "This is the tutorial notebook for how to use the stress determinator, prepared by **DMaS** and **Douglas Research Center**. \n",
    "\n",
    "**Data used:**\n",
    "\n",
    "All the data sourced from Dr. Wong's mouse neuron experiments in Douglas Research Center with two main categories used in our model training experiments:\n",
    "* Bullying mouse in the enclosure\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Adriandliu/Neural-Decoding-Project/blob/master/img/one_free.PNG?raw=true\" width=\"200\"/>\n",
    "\n",
    "* Bullying and defeated mice are both free to move in the cage\n",
    "\n",
    "<img src=\"https://github.com/Adriandliu/Neural-Decoding-Project/blob/master/img/two_free.PNG?raw=true\" width=\"150\"/>\n",
    "\n",
    "\n",
    "Data are mainly backed up and stored in Drive H:/Donghan's Project Data Backup/.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "In order to make this experiment reproducible, please make sure the following data are available for input:\n",
    "* Neuron activity data, primarily extracted from [CNMF-E](https://github.com/zhoupc/CNMF_E), available in CNMF-E folder\n",
    "* Mouse behavioral data, primarily extracted and labeled from [DeepLabCut](https://github.com/AlexEMG/DeepLabCut), available in DeepLabCut folder\n",
    "* Timestamp file that automatically generated by the camera and its application is file for aligning behavioral camera and neuron camera, available in the index format of mouse experiments date and time (./Raw data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessor import distOneF,distTwoF,fourPointTransform,locCoordConvert,ptsCoordConvert,align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read and preparation\n",
    "\n",
    "### Overall procedure:\n",
    "\n",
    "1. **Align** the frames between behavioral data and neuron data by timestamp file\n",
    "1. **Determine** the four corner coordinate points\n",
    "2. **Transform** the defeated behavioral data from pixel-based to centimeter-based\n",
    "3. **Transform** bullying mouse behavioral data\n",
    "4. Calculate the two mice real **distance**\n",
    "5. **Classify** the distance into two groups: \n",
    "    * interacted with bullying mouse: <10/15 cm\n",
    "    * no interaction: >10/15 cm\n",
    "\n",
    "\n",
    "### Some term explanations:\n",
    "\n",
    "* **gap_time**: In the raw video, there might be a gap time that the mouse is yet in the cage, so excludes this part helps the experimental veracity. Two ways to determine the gap time:\n",
    "    * CNMF-E folder may contain the gap information, such as 1056_SI_A_Substack (240-9603), means after 240 frames, the mouse then shows up in the cage\n",
    "    * Manually check the video one by one\n",
    "* **timestamp**: The file seperator is '\\t' and must exclude the first row (header) and rename them in [\"camNum\",\"frameNum\",\"sysClock\",\"buffer\"]\n",
    "* **video coordinates**: In the raw video, upper-left is (0,0), horizontal is X-axis, and vertical is Y-axis. While looking for the four corner points, screenshot one frame from the raw video, then import to image application, say photoshop/online RGB checker, to check its coordinates. (automation could be possible, but take time). Here are some corner points references for the experiment video:\n",
    "    * Two free-moving mice scenario: np.array([(40,60),(213,62), (205,405),(42,405)], np.float32)\n",
    "    * One free-moving mouse scenario: np.array([(85,100),(85,450), (425,440), (420,105)], np.float32)\n",
    "    \n",
    "    In addition, the actual size of the cage varies:\n",
    "       * Two free-moving mice: 22 * 44\n",
    "       * One free-moving mouse: 44 * 44\n",
    "       \n",
    "    but it all depends on which scenario the experiment chose\n",
    "\n",
    "* **bullying mouse position**: \n",
    "    * For one free-moving scenario, the bullying mouse is fixed inside the enclosure, its position is therefore fixed as well. In such case, we will use the central point of the enclosure as the bullying mouse position \n",
    "    * For two free-moving scenario, the bulling mouse position is read from DeepLabCut behaviral data. Therefore the distance calculation would be different\n",
    "    \n",
    "* **one-hot encoding**: Convert numerical labelled distance to one-hot format \n",
    "\n",
    "* **neuron_A/B**: \n",
    "    * neuron_A: no bullying mouse\n",
    "    * neuron_B: bullying mouse presents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_time_A = 240\n",
    "gap_time_B = 150\n",
    "\n",
    "dlc_A = pd.read_csv(\"//DMAS-WS2017-006/E/A RSync FungWongLabs/DLC_Data/1053 SI_A, Mar 22, 9 14 20/videos/\\\n",
    "1056 SI_A, Mar 22, 12 45 13DeepCut_resnet50_1053 SI_A, Mar 22, 9 14 20Jul31shuffle1_600000.h.csv\", skiprows = 2).iloc[gap_time_A:,]\n",
    "dlc_B = pd.read_csv(\"//DMAS-WS2017-006/E/A RSync FungWongLabs/DLC_Data/1053 SI_A, Mar 22, 9 14 20/videos/\\\n",
    "1056 SI_B, Mar 22, 12 52 59DeepCut_resnet50_1053 SI_A, Mar 22, 9 14 20Jul31shuffle1_600000.h.csv\", skiprows = 2).iloc[gap_time_B:,]\n",
    "\n",
    "\n",
    "neuron_A = pd.read_csv(\"//Dmas-ws2017-006/e/A RSync FungWongLabs/CNMF-E/1056/SI/1056_SI_A_Substack (240-9603)_source_extraction/frames_1_9364/LOGS_15-Sep_13_52_07/1056SI_A_240-9603.csv\", header = None).T\n",
    "neuron_B = pd.read_csv(\"//Dmas-ws2017-006/e/A RSync FungWongLabs/CNMF-E/1056/SI/1056_SI_B_source_extraction/frames_1_27256/LOGS_19-Apr_00_38_59/1056SI_B.csv\", header = None).T.iloc[gap_time_B:,]\n",
    "timestamp_A = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/Raw Data/Witnessing/female/Round 8/3_22_2019/H12_M45_S13/timestamp.dat\", \\\n",
    "sep='\\t', header = None, skiprows=1, names = [\"camNum\",\"frameNum\",\"sysClock\",\"buffer\"])\n",
    "timestamp_B = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/Raw Data/Witnessing/female/Round 8/3_22_2019/H12_M52_S59/timestamp.dat\", \\\n",
    "sep='\\t', header = None, skiprows=1, names = [\"camNum\",\"frameNum\",\"sysClock\",\"buffer\"])\n",
    "timestamp_A = timestamp_A[timestamp_A[\"frameNum\"]>=gap_time_A]\n",
    "timestamp_B = timestamp_B[timestamp_B[\"frameNum\"]>=gap_time_B]\n",
    "\n",
    "# f =  open(\"1056SIA_test_0.0001_0.3_lstm_10_pytorch.txt\",'w+')\n",
    "\n",
    "\n",
    "# IF ONE FREE-MOVING MOUSE\n",
    "msCam, behavCam = align(neuron_A, dlc_A, timestamp_A, gap_time_A)      # alignment[0] == aligned neurons_1053B; alignment[1] == aligned dlc_1053B\n",
    "pts = np.array([(85,100),(85,450), (425,440), (420,105)], np.float32)   # four corner points\n",
    "newLoc = locCoordConvert(behavCam,pts,44,44)                            # convert to new location data with new dimension\n",
    "referPt = ptsCoordConvert(pts, [400,270], 44, 44)[0]                    # convert bullying mouse location with new dimension\n",
    "dist = distOneF(newLoc, referPt)                                        # calculate distance between bullying and defeated mouse\n",
    "labeled = [1 if i < 10 else 0 for i in dist]                            # if dist < 15, label 1 (has interaction), else 0 (no interaction)\n",
    "\n",
    "\n",
    "# IF TWO FREE-MOVING MICE\n",
    "msCam, behavCam = align(neuron_A, dlc_A, timestamp_A, gap_time_A)      # alignment[0] == aligned neurons_1053B; alignment[1] == aligned dlc_1053B\n",
    "pts = np.array([(40,60),(213,62), (205,405),(42,405)], np.float32)   # four corner points\n",
    "newLoc = locCoordConvert(behavCam,pts,22,44)                            # convert to new location data with new dimension\n",
    "# referPt = ptsCoordConvert(pts, [400,270], 44, 44)[0]                    # convert bullying mouse location with new dimension\n",
    "dist = distTwoF(newLoc, \"head\")                                        # calculate distance between bullying and defeated mouse\n",
    "labeled = [1 if i < 15 else 0 for i in dist]                            # if dist < 15, label 1 (has interaction), else 0 (no interaction)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.concat([msCam, pd.DataFrame(labeled)], axis=1).dropna(axis = 0)\n",
    "data.columns = list(range(1,len(msCam.columns)+2))                      # avoid duplicate column name\n",
    "data = data.rename(columns={len(msCam.columns)+1:\"interaction\"})\n",
    "\n",
    "# One hot encoding\n",
    "one_hot = pd.get_dummies(data['interaction'])\n",
    "one_hot.columns = [\"interaction.a\", \"interaction.b\"]\n",
    "data = data.drop(\"interaction\", axis = 1).join(one_hot)\n",
    "\n",
    "frac = 0.3\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(data[list(range(1,len(data.columns)-1))], data[[\"interaction.a\", \"interaction.b\"]], test_size=frac, random_state=0)\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "x_train = x_train.drop(1, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for DL model\n",
    "\n",
    "Initialize model hyper-parameters and transform to torch data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 1\n",
    "input_size = len(x_train.columns)\n",
    "hidden_size = len(x_train.columns)\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "dropout = 0.3\n",
    "learning_rate = 0.003\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "x_train_tensor = torch.from_numpy(np.array(x_train)).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(np.array(y_train)).long().to(device)\n",
    "train = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train,batch_size=batch_size, shuffle=True)\n",
    "total_step = len(train_loader)\n",
    "\n",
    "x_test_tensor = torch.from_numpy(np.array(x_test)).float()\n",
    "y_test_tensor = torch.from_numpy(np.array(y_test)).float()\n",
    "test = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    return torch.tensor(seq, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional RNN  -  LSTM/GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(model, input_size, hidden_size, num_layers, dropout = 0.3, batch_first=True, bidirectional=True):\n",
    "    if model == 'GRU':\n",
    "        return nn.GRU(input_size, hidden_size, num_layers, dropout = dropout, batch_first=True, bidirectional=True)\n",
    "    else:\n",
    "        return nn.LSTM(input_size, hidden_size, num_layers,dropout = dropout, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, model, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.model = model\n",
    "        self.lstm = RNN(self.model, input_size, hidden_size, num_layers, dropout = 0.3, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss and Optimizer functions\n",
    "\n",
    "* **Cross-Entropy**: Loss for classification\n",
    "* **Adam**: Efficient and outperforms many others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiRNN('LSTM', input_size, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = prepare_sequence(images).reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = prepare_sequence(labels).long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden = model(images)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Neuron and Behavior Data\n",
    "This notebook contains the preprocessor for align the neuron and behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distOneF(data,referencePts):\n",
    "    '''\n",
    "    distance calculation for one free-moving mouse data\n",
    "\n",
    "    referencePts: two elements array/list/tuple\n",
    "        bullying mouse position in pixels\n",
    "    '''\n",
    "    fixedX = referencePts[0]\n",
    "    fixedY = referencePts[1]\n",
    "    x = data.x\n",
    "    y = data.y\n",
    "    dist = np.sqrt((x - fixedX)**2 + (y - fixedY)**2)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def distTwoF(data, pos):\n",
    "    '''\n",
    "    data: dlc data\n",
    "    pos: \"head\", \"body\", \"tail\" for distance calculation\n",
    "    '''\n",
    "    if pos == \"head\":\n",
    "        return np.sqrt((data[\"x\"] - data[\"x.3\"])**2 + (data[\"y\"] - data[\"y.3\"])**2)\n",
    "    elif pos == \"body\":\n",
    "        return np.sqrt((data[\"x.1\"] - data[\"x.4\"])**2 + (data[\"y.1\"] - data[\"y.4\"])**2)\n",
    "    else:\n",
    "        return np.sqrt((data[\"x.2\"] - data[\"x.5\"])**2 + (data[\"y.2\"] - data[\"y.5\"])**2)\n",
    "\n",
    "\n",
    "\n",
    "def fourPointTransform(pts, dwd, dht, image = None):\n",
    "    '''\n",
    "    Task: Transform image by its corner four points.\n",
    "\n",
    "    PARAMETERS:\n",
    "    -----------\n",
    "    image: array, Optional if intend to transform an image\n",
    "        image matrix\n",
    "\n",
    "    pts: list, tuple, array of list\n",
    "        coordinates of the top-left, top-right, bottom-right, and bottom-left points\n",
    "\n",
    "    dwd: width of the destination image\n",
    "\n",
    "    dht: height of the destination image\n",
    "    '''\n",
    "    dst = np.array([[0, 0], [dwd-1, 0], [dwd-1, dht-1], [0, dht-1]], dtype=\"float32\")\n",
    "\n",
    "    # Transformation matrix\n",
    "    tmat = cv2.getPerspectiveTransform(pts, dst)\n",
    "\n",
    "    if image is None:\n",
    "        return tmat\n",
    "    else:\n",
    "        # Apply the matrix\n",
    "        warped = cv2.warpPerspective(image, tmat, (dwd, dht))\n",
    "        return warped, tmat\n",
    "\n",
    "\n",
    "def locCoordConvert(data, pts, dwd, dht):\n",
    "    '''\n",
    "    Task: Convert mouse location data to prospective coordicates after correcting coord system.\n",
    "\n",
    "    PARAMETERS:\n",
    "    -----------\n",
    "    data: DataFrame\n",
    "        original location data\n",
    "\n",
    "    pts: list, tuple, array of list\n",
    "        coordinates of the top-left, top-right, bottom-right, and bottom-left points\n",
    "\n",
    "    dwd: width of the destination image\n",
    "\n",
    "    dht: height of the destination image\n",
    "    '''\n",
    "    # Transposed transformation matrix\n",
    "    tp_tmat = fourPointTransform(pts, dwd, dht)\n",
    "    columns = [i for i in data.columns if i[0] == \"x\" or i[0] == \"y\"]\n",
    "    transformed = pd.DataFrame()\n",
    "    # x, y, 1\n",
    "    for i in range(0,len(columns),2):\n",
    "        temp = pd.concat([data[[columns[i],columns[i+1]]],\n",
    "                        pd.DataFrame([1]*len(data))], axis = 1, join = \"inner\").values\n",
    "        transform = pd.DataFrame(np.dot(temp, tp_tmat)[:,:2])\n",
    "\n",
    "        transformed = pd.concat([transformed, transform], ignore_index = True, axis=1)\n",
    "    transformed.columns = columns\n",
    "    # Since (transformed head)^T = (transformation matrix)(head)^T, and (AB)^T = B^TA^T\n",
    "    # Transformed head = head(transformation matrix)^T\n",
    "\n",
    "    return transformed\n",
    "\n",
    "def ptsCoordConvert(refPts, pts, dwd, dht):\n",
    "    '''\n",
    "    Task: Convert user-specifed points to prospective coordicates after correcting coord system.\n",
    "\n",
    "    PARAMETERS:\n",
    "    -----------\n",
    "    refPts: list, tuple, array of list\n",
    "        coordinates of the top-left, top-right, bottom-right, and bottom-left points\n",
    "\n",
    "    pts: list, tuple, array of list\n",
    "        coordinates of points to convert\n",
    "\n",
    "    dwd: width of the destination image\n",
    "\n",
    "    dht: height of the destination image\n",
    "    '''\n",
    "    # Transformation matrix\n",
    "    tmat = fourPointTransform(refPts, dwd, dht)\n",
    "    transformedPts = []\n",
    "    # Converse mutiple points\n",
    "    if np.array(pts).shape != (2,):\n",
    "        for i in pts:\n",
    "            i.append(1)\n",
    "            transformedPts.append(list(np.dot(tmat, i)[:2]))\n",
    "    # Single point\n",
    "    else:\n",
    "        pts.append(1)\n",
    "        transformedPts.append(list(np.dot(tmat, pts)[:2]))\n",
    "\n",
    "    return transformedPts\n",
    "def align(neuron_data, dlc_data, timestamp, gap_time):\n",
    "    '''\n",
    "    Task: align neuron data and dlc data based on the corresponding timestamp.dat. The alignment is followed by frame number\n",
    "\n",
    "    PARAMETERS:\n",
    "    -----------\n",
    "    neuron_data: cnmfe data, transposed\n",
    "\n",
    "    dlc_data: deeplabcut data\n",
    "\n",
    "    timestamp: timestamp file in the specific mouse folder\n",
    "\n",
    "    return: sorted msCam, sorted behavCam\n",
    "    '''\n",
    "    new_order = []\n",
    "    # Check the diff between cam and behav in timestamp.\n",
    "    camNum = list(set(timestamp.camNum))\n",
    "    redundant = list(set(timestamp[timestamp[\"camNum\"]==camNum[0]][\"frameNum\"]) - set(timestamp[timestamp[\"camNum\"]==camNum[1]][\"frameNum\"])) # may cause NAN value afterwards, so remove it now\n",
    "    for i, index in zip(timestamp[\"frameNum\"].values, timestamp.index):\n",
    "        if i not in redundant:\n",
    "            continue\n",
    "        else:\n",
    "            timestamp = timestamp.drop(index)\n",
    "\n",
    "    # We do not need coords column\n",
    "    dlc_data = dlc_data.drop(columns = \"coords\", axis = 1)\n",
    "    # For length of dlc and neuron data is not the same, take out the redundant data (may be caused by lack of data while integrating behavioral video)\n",
    "\n",
    "    min_len = min(len(neuron_data), len(dlc_data), len(timestamp), len(timestamp[timestamp[\"camNum\"]==camNum[0]]), len(timestamp[timestamp[\"camNum\"]==camNum[0]]))\n",
    "    neuron_data = neuron_data.iloc[0:min_len:]\n",
    "    dlc_data = dlc_data.iloc[0:min_len:]\n",
    "    timestamp[\"frameNum\"] = timestamp[\"frameNum\"] - gap_time + 1 #change of index\n",
    "    timestamp = timestamp[timestamp[\"frameNum\"]<=min_len]\n",
    "    timestamp.index = range(0,len(timestamp))\n",
    "\n",
    "    try:\n",
    "        for ms, behav in zip(timestamp[\"camNum\"],timestamp[\"frameNum\"]):\n",
    "            if ms == 0:\n",
    "                new_order.append(neuron_data.iloc[behav-1].values) # -1 becuase frameNum start from 1 while neuron_data start from 0\n",
    "            else:\n",
    "                new_order.append(dlc_data.iloc[behav-1].values)\n",
    "    except IndexError:\n",
    "        print(\"Neuron data and dlc data are not in the same length, fix by checking the video length for each\")\n",
    "    merge_data = pd.concat([pd.DataFrame(new_order), timestamp[[\"camNum\",\"frameNum\"]]], axis = 1).sort_values(by = \"frameNum\")\n",
    "    msCam = merge_data[merge_data[\"camNum\"]==camNum[0]].dropna(axis = 1).drop(columns = [\"camNum\",\"frameNum\"], axis = 1)\n",
    "    msCam.index = range(1,len(msCam)+1) # for later concatenate\n",
    "    msCam.columns = range(0,len(msCam.columns))\n",
    "\n",
    "    behavCam = merge_data[merge_data[\"camNum\"]==camNum[1]].dropna(axis = 1).drop(columns = [\"camNum\",\"frameNum\"], axis = 1)\n",
    "    behavCam.columns = dlc_data.columns\n",
    "    behavCam.index = range(0,len(behavCam)) # for later concatenate\n",
    "    return (msCam,behavCam)\n",
    "\n",
    "def dataPrep(filename, split_frac, scenario, corner_pts, cage_dim, refer_pt, dist_thres, gap_time, batch_size):\n",
    "    '''\n",
    "    Task: Prepare data for feeding DL model\n",
    "\n",
    "    PARAMETERS:\n",
    "    ------------\n",
    "\n",
    "    filename: dict\n",
    "        Key: neuron_A, neuron_B, dlc_A, dlc_B, timestamp_A, timestamp_B\n",
    "        Value: their corresponding file path and names\n",
    "    split_frac: decimal\n",
    "        split frac for train_val_test split\n",
    "    scenario: str\n",
    "        \"one\": one free-moving mouse or \"two\": two free-moving mice\n",
    "    corner_pts: numpy array with data type np.float32\n",
    "        cage four corner coordinate points, upper left, upper right, lower right, lower left\n",
    "    cage_dim: int\n",
    "        cage dimension in centimeters\n",
    "    refer_pt: tuple, list\n",
    "        bullying mouse position in pixels\n",
    "    dist_thres: int\n",
    "        the distance threshold between two mice, <threshold = interacted, >threshold = not interacted\n",
    "    gap_time: int\n",
    "        time of no mouse shows up in the cage, in frames\n",
    "    batch_size: int\n",
    "        model batch size\n",
    "    '''\n",
    "\n",
    "    gap_time_A = gap_time\n",
    "    gap_time_B = gap_time\n",
    "\n",
    "    dlc_A = pd.read_csv(filename['dlc_A'], skiprows = 2).iloc[gap_time_A:,]\n",
    "    dlc_B = pd.read_csv(filename['dlc_B'], skiprows = 2).iloc[gap_time_B:,]\n",
    "\n",
    "\n",
    "    neuron_A = pd.read_csv(filename['neuron_A'], header = None).T\n",
    "    neuron_B = pd.read_csv(filename['neuron_B'], header = None).T.iloc[gap_time_B:,]\n",
    "    timestamp_A = pd.read_csv(filename['timestamp_A'], \\\n",
    "    sep='\\t', header = None, skiprows=1, names = [\"camNum\",\"frameNum\",\"sysClock\",\"buffer\"])\n",
    "    timestamp_B = pd.read_csv(filename['timestamp_B'], \\\n",
    "    sep='\\t', header = None, skiprows=1, names = [\"camNum\",\"frameNum\",\"sysClock\",\"buffer\"])\n",
    "    timestamp_A = timestamp_A[timestamp_A[\"frameNum\"]>=gap_time_A]\n",
    "    timestamp_B = timestamp_B[timestamp_B[\"frameNum\"]>=gap_time_B]\n",
    "\n",
    "\n",
    "    msCam, behavCam = align(neuron_A, dlc_A, timestamp_A, gap_time_A)      # alignment[0] == aligned neurons_1053B; alignment[1] == aligned dlc_1053B\n",
    "    pts = corner_pts                                                       # four corner points\n",
    "    newLoc = locCoordConvert(behavCam,pts,cage_dim[0],cage_dim[1])                            # convert to new location data with new dimension\n",
    "    if scenari == \"one\":\n",
    "        referPt = ptsCoordConvert(pts, refer_pt, cage_dim[0],cage_dim[1])[0]                    # convert bullying mouse location with new dimension\n",
    "        dist = distOneF(newLoc, referPt)\n",
    "    else:\n",
    "        dist = distTwoF(newLoc, \"head\")                                  # calculate distance between bullying and defeated mouse\n",
    "    labeled = [1 if i < dist_thres else 0 for i in dist]                            # if dist < 15, label 1 (has interaction), else 0 (no interaction)\n",
    "\n",
    "\n",
    "    data = pd.concat([msCam, pd.DataFrame(labeled)], axis=1).dropna(axis = 0)\n",
    "    data.columns = list(range(1,len(msCam.columns)+2))                      # avoid duplicate column name\n",
    "    data = data.rename(columns={len(msCam.columns)+1:\"interaction\"})\n",
    "\n",
    "    # One hot encoding\n",
    "    one_hot = pd.get_dummies(data['interaction'])\n",
    "    one_hot.columns = [\"interaction.a\", \"interaction.b\"]\n",
    "    data = data.drop(\"interaction\", axis = 1).join(one_hot)\n",
    "\n",
    "    frac = split_frac\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "            train_test_split(data[list(range(1,len(data.columns)-1))], data[[\"interaction.a\", \"interaction.b\"]], test_size=frac, random_state=0)\n",
    "\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    x_train = x_train.drop(1, axis = 1)\n",
    "    y_embedding = np.zeros((len(x_train), len(x_train.columns)))\n",
    "\n",
    "    x_train_tensor = torch.from_numpy(np.array(x_train)).float().to(device)\n",
    "    y_train_tensor = torch.from_numpy(np.array(y_train)).long().to(device)\n",
    "    train = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "\n",
    "    x_val_tensor = torch.from_numpy(np.array(x_val)).float()\n",
    "    y_val_tensor = torch.from_numpy(np.array(y_val)).float()\n",
    "    val= TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=test,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    x_test_tensor = torch.from_numpy(np.array(x_test)).float()\n",
    "    y_test_tensor = torch.from_numpy(np.array(y_test)).float()\n",
    "    test = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = {'dlc_A':\"//DMAS-WS2017-006/E/A RSync FungWongLabs/DLC_Data/1053 SI_A, Mar 22, 9 14 20/videos/\\\n",
    "1056 SI_A, Mar 22, 12 45 13DeepCut_resnet50_1053 SI_A, Mar 22, 9 14 20Jul31shuffle1_600000.h.csv\",\n",
    "'dlc_B':\"//DMAS-WS2017-006/E/A RSync FungWongLabs/DLC_Data/1053 SI_A, Mar 22, 9 14 20/videos/\\\n",
    "1056 SI_B, Mar 22, 12 52 59DeepCut_resnet50_1053 SI_A, Mar 22, 9 14 20Jul31shuffle1_600000.h.csv\",\n",
    "'neuron_A':\"//Dmas-ws2017-006/e/A RSync FungWongLabs/CNMF-E/1056/SI/1056_SI_A_Substack (240-9603)_source_extraction/frames_1_9364/LOGS_15-Sep_13_52_07/1056SI_A_240-9603.csv\",\n",
    "'neuron_B':\"//Dmas-ws2017-006/e/A RSync FungWongLabs/CNMF-E/1056/SI/1056_SI_B_source_extraction/frames_1_27256/LOGS_19-Apr_00_38_59/1056SI_B.csv\",\n",
    "'timestamp_A':\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/Raw Data/Witnessing/female/Round 8/3_22_2019/H12_M45_S13/timestamp.dat\",\n",
    "'timestamp_B':\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/Raw Data/Witnessing/female/Round 8/3_22_2019/H12_M52_S59/timestamp.dat\"}\n",
    "    split_frac = 0.3\n",
    "    scenario = 'one'\n",
    "    corner_pts = np.array([(85,100),(85,450), (425,440), (420,105)], np.float32)\n",
    "    cage_dim = [44,44]\n",
    "    refer_pt = [400,270]\n",
    "    dist_thres = 15\n",
    "    gap_time = 270\n",
    "    batch_size = 128\n",
    "\n",
    "    dataPrep(filename, split_frac, scenario, corner_pts, cage_dim, refer_pt, dist_thres, gap_time, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Read\n",
    "All the path files that I used stored in filePath.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap time: the time that no mouse presents in the enclosure. For two free-moving mice, ususally 0.\n",
    "gap_time_A = 0\n",
    "gap_time_B = 0\n",
    "\n",
    "neuron_A = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/CNMF-E/1033/Def2/1033_Def2_B_source_extraction/frames_1_18166/LOGS_18-Nov_16_02_51/1033_Def2_B.csv\", header = None).T\n",
    "neuron_B = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/CNMF-E/1033/Def2/1033_Def2_A_source_extraction/frames_1_13008/LOGS_20-Nov_12_51_19/1033_Def2_A.csv\", header = None).T\n",
    "dlc_A = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/DLC_Data/1033 Def2(2)_A, Aug 8, 13 5 2/videos/1033 Def2(2)_A, Aug 8, 13 5 2DeepCut_resnet50_Social_DefeatJul11shuffle1_120000.h.csv\", skiprows = 2)\n",
    "dlc_B = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/DLC_Data/1033 Def2(2)_A, Aug 8, 13 5 2/videos/1033 Def2(2)_B, Aug 8, 13 13 8DeepCut_resnet50_Social_DefeatJul11shuffle1_120000.h.csv\", skiprows = 2)\n",
    "timestamp_A = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/Raw Data/Regular Social Defeat/Round 5/8_8_2018/H13_M5_S2/timestamp.dat\", \\\n",
    "sep='\\t', header = None, skiprows=1, names = [\"camNum\",\"frameNum\",\"sysClock\",\"buffer\"])\n",
    "timestamp_B = pd.read_csv(\"//DMAS-WS2017-006/H/Donghan's Project Data Backup/Raw Data/Regular Social Defeat/Round 5/8_8_2018/H13_M13_S8/timestamp.dat\", \\\n",
    "sep='\\t', header = None, skiprows=1, names = [\"camNum\",\"frameNum\",\"sysClock\",\"buffer\"])\n",
    "timestamp_A = timestamp_A[timestamp_A[\"frameNum\"]>=gap_time_A]\n",
    "timestamp_B = timestamp_B[timestamp_B[\"frameNum\"]>=gap_time_B]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If use bullying mouse in the enclosure data (enclosure point fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "    \n",
    "\n",
    "*Attention*: **pts** is subject to change, which is the corner point of the original video, by pixels. \n",
    "\n",
    "**cagewidth** and **cageheight** is the real dimension of the cage, by centimeter.\n",
    "\n",
    "Order: top-right, top-left, bottom-left, bottom-right. \n",
    "\n",
    "Note that the (0,0) origin pixel point is on the top-right of the video, horizontal is X, vertical is Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(data,referencePts):\n",
    "    fixedX = referencePts[0]\n",
    "    fixedY = referencePts[1]\n",
    "    x = data.x\n",
    "    y = data.y\n",
    "    dist = np.sqrt((x - fixedX)**2 + (y - fixedY)**2)\n",
    "    return dist\n",
    "\n",
    "pts = np.array([(85,100),(85,450), (425,440), (420,105)], np.float32)   # four corner points\n",
    "newLoc = locCoordConvert(behavCam,pts,44,44)                            # convert to new location data with new dimension\n",
    "referPt = ptsCoordConvert(pts, [400,270], 44, 44)[0]                    # convert bullying mouse location with new dimension\n",
    "dist = distance(newLoc, referPt)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If use two free-moving mice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distTwoF(data, pos):\n",
    "    '''\n",
    "    data: dlc data\n",
    "    pos: \"head\", \"body\", \"tail\" for distance calculation\n",
    "    '''\n",
    "    if pos == \"head\":\n",
    "        return np.sqrt((data[\"x\"] - data[\"x.3\"])**2 + (data[\"y\"] - data[\"y.3\"])**2)\n",
    "    elif pos == \"body\":\n",
    "        return np.sqrt((data[\"x.1\"] - data[\"x.4\"])**2 + (data[\"y.1\"] - data[\"y.4\"])**2)\n",
    "    else:\n",
    "        return np.sqrt((data[\"x.2\"] - data[\"x.5\"])**2 + (data[\"y.2\"] - data[\"y.5\"])**2)\n",
    "\n",
    "\n",
    "cagewidth = 22\n",
    "cageheight = 44\n",
    "distThreshold = 15\n",
    "\n",
    "msCam, behavCam = align(neuron_A, dlc_A, timestamp_A, gap_time_A)      # alignment[0] == aligned neurons_1053B; alignment[1] == aligned dlc_1053B\n",
    "pts = np.array([(40,60),(213,62), (205,405),(42,405)], np.float32)   # four corner points\n",
    "newLoc = locCoordConvert(behavCam,pts,cagewidth, cageheight)           # convert to new location data with new dimension\n",
    "# referPt = ptsCoordConvert(pts, [400,270], 44, 44)[0]                    # convert bullying mouse location with new dimension\n",
    "dist = distTwoF(newLoc, \"head\")                                        # calculate distance between bullying and defeated mouse\n",
    "labeled = [1 if i < distThreshold else 0 for i in dist]                            # if dist < 15, label 1 (has interaction), else 0 (no interaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.concat([msCam, pd.DataFrame(labeled)], axis=1).dropna(axis = 0)\n",
    "data.columns = list(range(1,len(msCam.columns)+2))                      # avoid duplicate column name\n",
    "data = data.rename(columns={len(msCam.columns)+1:\"interaction\"})\n",
    "\n",
    "(data.groupby(by = \"interaction\").count())\n",
    "# One hot encoding\n",
    "one_hot = pd.get_dummies(data['interaction'])\n",
    "one_hot.columns = [\"interaction.a\", \"interaction.b\"]\n",
    "data = data.drop(\"interaction\", axis = 1).join(one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional write to csv file\n",
    "\n",
    "file_name by user choice \n",
    "\n",
    "file ouputs to the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-19d1dd04fa10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#OR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_name' is not defined"
     ]
    }
   ],
   "source": [
    "data.to_csv(file_name, sep='\\t')\n",
    "#OR \n",
    "data.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
